"""
Project Ana - Sentiment + Gemini Backend

Requirements:
  pip install flask flask-cors transformers torch scipy python-dotenv google-genai==1.49.0

Env:
  .env in SAME FOLDER as this file:

    GEMINI_API_KEY=your_real_key_here
"""

import os
import json
import logging
import random
from typing import Dict, List
from pathlib import Path

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax

from dotenv import load_dotenv
from google import genai

# ==========================
# üìÇ Paths & Flask Setup
# ==========================

BASE_DIR = Path(__file__).resolve().parent

app = Flask(
    __name__,
    template_folder=str(BASE_DIR / "templates"),
    static_folder=str(BASE_DIR / "static"),
)
CORS(app)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("project_ana")

# Data dir: <project>/Data
JSON_DATA_DIR = BASE_DIR / "Data"
os.makedirs(JSON_DATA_DIR, exist_ok=True)
logger.info(f"üìÅ JSON data dir: {JSON_DATA_DIR}")

# ==========================
# üîê Gemini Setup (google-genai 1.49.0)
# ==========================

env_path = BASE_DIR / ".env"
load_dotenv(env_path)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise RuntimeError(
        f"‚ùå GEMINI_API_KEY not found. Create {env_path} with GEMINI_API_KEY=your_key"
    )

try:
    gemini_client = genai.Client(api_key=GEMINI_API_KEY)
    _ = list(gemini_client.models.list())  # sanity check
    logger.info("‚úÖ Gemini client initialized successfully.")
except Exception as e:
    raise RuntimeError(f"‚ùå Failed to initialize Gemini client: {e}")

# ==========================
# ü§ñ Sentiment Model Setup
# ==========================

MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment"
device = "cuda" if torch.cuda.is_available() else "cpu"

logger.info(f"üîß Loading sentiment model: {MODEL_NAME}")
logger.info(f"üíª Using device: {device}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)
model.eval()

logger.info("‚úÖ Sentiment model loaded successfully.")

# Cache for analyzed results
sentiment_cache: Dict[str, object] = {}

# ==========================
# üß† Sentiment Logic (Improved)
# ==========================

def sentiment_analysis_improved(text: str) -> Dict:
    """
    1-5 style score with robust neutral handling.
    """
    if not text or not str(text).strip():
        return {
            "score": 3.0,
            "label": "neutral",
            "confidence": 1.0,
            "probabilities": {"neg": 0.0, "neu": 1.0, "pos": 0.0},
        }

    inputs = tokenizer(
        str(text),
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=512,
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits[0].cpu().numpy()
    probs = softmax(logits)
    p_neg, p_neu, p_pos = probs

    # Weighted 1‚Äì5 score
    score = 1 * p_neg + 3 * p_neu + 5 * p_pos

    # Improved labeling
    if p_neu > 0.5:
        label = "neutral"
    elif p_neg > 0.2 and p_pos > 0.2:
        label = "neutral"
    elif score >= 3.8:
        label = "positive"
    elif score <= 2.2:
        label = "negative"
    else:
        max_prob = max(p_neg, p_neu, p_pos)
        if max_prob < 0.5:
            label = "neutral"
        elif p_neu > p_neg and p_neu > p_pos:
            label = "neutral"
        elif score > 3.0:
            label = "slightly positive"
        else:
            label = "slightly negative"

    confidence = max(p_neg, p_neu, p_pos)

    return {
        "score": round(float(score), 2),
        "label": label,
        "confidence": round(float(confidence), 3),
        "probabilities": {
            "neg": round(float(p_neg), 3),
            "neu": round(float(p_neu), 3),
            "pos": round(float(p_pos), 3),
        },
    }

# ==========================
# üì¶ JSON Text Extraction
# ==========================

def extract_text_from_review(review: dict) -> str:
    """
    Aggressive text extraction.
    Critical: if this returns "", that item gets NO sentiment.
    Handles:
    - title / text
    - review_text, full_text, etc
    - nested text objects
    - plain string reviews
    - fallback: concat all string fields
    """
    # If it's literally a string, treat as the review body
    if isinstance(review, str):
        return review.strip()

    if not isinstance(review, dict):
        return ""

    # Priority 1: title + text
    title = (review.get("title") or "").strip()
    text = (review.get("text") or "").strip()
    if title and text:
        return f"{title}. {text}"
    if text:
        return text
    if title:
        return title

    # Priority 2: common fields
    candidate_keys = [
        "full_text",
        "fullText",
        "review_text",
        "reviewText",
        "body",
        "comment",
        "review",
        "content",
        "message",
        "description",
    ]
    for key in candidate_keys:
        val = review.get(key)
        if isinstance(val, str) and val.strip():
            return val.strip()

    # Priority 3: nested "text": { "raw": "..." }
    nested = review.get("text")
    if isinstance(nested, dict):
        for _, v in nested.items():
            if isinstance(v, str) and v.strip():
                return v.strip()

    # Priority 4: generic heuristic ‚Äî
    # grab any string-ish fields that look like sentences and join them
    parts = []
    for k, v in review.items():
        if isinstance(v, str):
            s = v.strip()
            # avoid obvious non-review fields like IDs
            if s and len(s) > 8 and not k.lower() in ["id", "store_id", "user_id"]:
                parts.append(s)
    if parts:
        return " ".join(parts)

    return ""

# ==========================
# üì¶ JSON Processing
# ==========================

def process_nested_json(json_data: dict, filename: str) -> List[Dict]:
    """
    Handles location-structured JSON:
    {
      "states": [
        {
          "state": "...",
          "cities": [
            {
              "city": "...",
              "stores": [
                {
                  "store_id": "...",
                  "store_name": "...",
                  "address": "...",
                  "reviews": [ {...}, "text", ... ]
                }
              ]
            }
          ]
        }
      ]
    }
    """
    results = []
    if "states" not in json_data:
        return results

    logger.info(f"üì• Processing nested JSON from {filename}")

    for state_obj in json_data.get("states", []):
        state = state_obj.get("state", "Unknown")

        for city_obj in state_obj.get("cities", []):
            city = city_obj.get("city", "Unknown")

            for store in city_obj.get("stores", []):
                store_info = {
                    "store_id": store.get("store_id"),
                    "store_name": store.get("store_name"),
                    "address": store.get("address"),
                    "city": store.get("city", city),
                    "state": store.get("state", state),
                }

                for review in store.get("reviews", []):
                    text = extract_text_from_review(review)
                    if not text:
                        continue

                    sent = sentiment_analysis_improved(text)

                    # Normalize review dict if it's not one
                    review_dict = review if isinstance(review, dict) else {}

                    results.append({
                        **store_info,
                        "_source_file": filename,
                        "_structure": "nested",
                        "title": review_dict.get("title", ""),
                        "text": review_dict.get("text", ""),
                        "full_text": text,
                        "original_rating": review_dict.get("rating"),
                        "original_sentiment": review_dict.get("sentiment"),
                        "date": review_dict.get("date", ""),
                        "sentiment_score": sent["score"],
                        "sentiment_label": sent["label"],
                        "sentiment_confidence": sent["confidence"],
                        "sentiment_probabilities": sent["probabilities"],
                    })

    logger.info(f"‚úÖ Nested parsed from {filename}: {len(results)} rows")
    return results

def process_flat_json(json_data, filename: str) -> List[Dict]:
    results = []

    if isinstance(json_data, list):
        items = json_data
    elif isinstance(json_data, dict):
        items = json_data.get(
            "posts",
            json_data.get(
                "data",
                json_data.get(
                    "comments",
                    json_data.get(
                        "reviews",
                        json_data.get("items", [json_data]),
                    ),
                ),
            ),
        )
    else:
        return results

    if not isinstance(items, list):
        items = [items]

    logger.info(f"üì• Processing flat JSON from {filename}: {len(items)} items")

    for item in items:
        # allow plain string items as reviews
        if isinstance(item, str):
            text = item.strip()
        elif isinstance(item, dict):
            text = extract_text_from_review(item)
        else:
            continue

        if not text:
            continue

        sent = sentiment_analysis_improved(text)
        base = item if isinstance(item, dict) else {"raw": item}

        results.append({
            **base,
            "_source_file": filename,
            "_structure": "flat",
            "full_text": text,
            "sentiment_score": sent["score"],
            "sentiment_label": sent["label"],
            "sentiment_confidence": sent["confidence"],
            "sentiment_probabilities": sent["probabilities"],
        })

    logger.info(f"‚úÖ Flat parsed from {filename}: {len(results)} rows")
    return results

def load_and_process_json_file(path: Path) -> List[Dict]:
    filename = path.name
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)

        if isinstance(data, dict) and "states" in data:
            return process_nested_json(data, filename)
        return process_flat_json(data, filename)

    except Exception as e:
        logger.error(f"‚ùå Error loading {filename}: {e}")
        return []

# ==========================
# üåê Routes
# ==========================

@app.route("/")
def home():
    return render_template("project_ana.html")

@app.route("/health", methods=["GET"])
def health():
    return jsonify({
        "status": "healthy",
        "model": MODEL_NAME,
        "device": device,
        "gemini_client": True,
    }), 200

@app.route("/load_and_analyze", methods=["POST"])
def load_and_analyze():
    try:
        json_files = list(JSON_DATA_DIR.glob("*.json"))

        if not json_files:
            return jsonify({
                "error": f"No JSON files found in '{JSON_DATA_DIR}'.",
                "results": [],
                "summary": {},
            }), 404

        all_results: List[Dict] = []
        file_details = []

        for path in json_files:
            results = load_and_process_json_file(path)
            all_results.extend(results)
            file_details.append({
                "filename": path.name,
                "reviews_found": len(results),
                "structure": results[0]["_structure"] if results else "unknown",
            })

        # Deduplicate by full_text
        seen = set()
        unique = []
        for r in all_results:
            key = r.get("full_text") or r.get("text") or ""
            if key and key not in seen:
                seen.add(key)
                unique.append(r)

        all_results = unique
        logger.info(f"‚úÖ Total records after dedupe: {len(all_results)}")

        # Summary
        if all_results:
            labels = {}
            total_score = 0.0
            total_conf = 0.0
            states, cities, stores = set(), set(), set()

            for r in all_results:
                lbl = r["sentiment_label"]
                labels[lbl] = labels.get(lbl, 0) + 1
                total_score += r["sentiment_score"]
                total_conf += r["sentiment_confidence"]

                if r.get("state"):
                    states.add(r["state"])
                if r.get("city"):
                    cities.add(r["city"])
                if r.get("store_id"):
                    stores.add(r["store_id"])

            summary = {
                "total_comments": len(all_results),
                "files_analyzed": len(json_files),
                "file_details": file_details,
                "label_distribution": labels,
                "average_score": round(total_score / len(all_results), 2),
                "average_confidence": round(total_conf / len(all_results), 3),
                "geographic_coverage": {
                    "states": len(states),
                    "cities": len(cities),
                    "stores": len(stores),
                    "state_names": sorted(states),
                    "city_names": sorted(cities),
                    "store_ids": sorted(stores),
                },
            }
        else:
            summary = {
                "total_comments": 0,
                "files_analyzed": len(json_files),
                "file_details": file_details,
            }

        sentiment_cache["latest"] = all_results
        sentiment_cache["summary"] = summary

        return jsonify({"results": all_results, "summary": summary}), 200

    except Exception as e:
        logger.error(f"‚ùå /load_and_analyze error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route("/filter_reviews", methods=["POST"])
def filter_reviews():
    try:
        if "latest" not in sentiment_cache:
            return jsonify({"error": "No data loaded. Call /load_and_analyze first."}), 400

        data = request.get_json() or {}
        reviews = sentiment_cache["latest"]

        sentiment_filter = data.get("sentiment", "any").lower()
        min_score = float(data.get("min_score", 1.0))
        max_score = float(data.get("max_score", 5.0))
        state = data.get("state")
        city = data.get("city")
        store = data.get("store_id")

        filtered = []
        for r in reviews:
            if sentiment_filter != "any" and sentiment_filter not in r["sentiment_label"].lower():
                continue
            if not (min_score <= r["sentiment_score"] <= max_score):
                continue
            if state and r.get("state") != state:
                continue
            if city and r.get("city") != city:
                continue
            if store and r.get("store_id") != store:
                continue
            filtered.append(r)

        summary = {"total_filtered": len(filtered)}
        if filtered:
            labels = {}
            total_score = 0.0
            for r in filtered:
                lbl = r["sentiment_label"]
                labels[lbl] = labels.get(lbl, 0) + 1
                total_score += r["sentiment_score"]
            summary.update({
                "label_distribution": labels,
                "average_score": round(total_score / len(filtered), 2),
            })

        return jsonify({"results": filtered, "summary": summary}), 200

    except Exception as e:
        logger.error(f"‚ùå /filter_reviews error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route("/analyze", methods=["POST"])
def analyze_single():
    try:
        data = request.get_json() or {}
        text = (data.get("text") or "").strip()
        if not text:
            return jsonify({"error": "Missing 'text'"}), 400

        result = sentiment_analysis_improved(text)
        result["text"] = text
        return jsonify(result), 200

    except Exception as e:
        logger.error(f"‚ùå /analyze error: {e}")
        return jsonify({"error": str(e)}), 500

# ==========================
# üí¨ /chat with shortcuts
# ==========================

@app.route("/chat", methods=["POST"])
def chat():
    if "latest" not in sentiment_cache or "summary" not in sentiment_cache:
        return jsonify({"response": "Please load the data first from the dashboard."}), 400

    data = request.get_json() or {}
    message = (data.get("message") or "").strip()
    use_latest = data.get("use_latest_data", True)

    if not message:
        return jsonify({"response": "Please enter a question."}), 400

    all_reviews: List[Dict] = sentiment_cache["latest"]
    summary: Dict = sentiment_cache["summary"]
    lower = message.lower()

    geo = summary.get("geographic_coverage", {})
    city_names = geo.get("city_names", []) or []
    state_names = geo.get("state_names", []) or []

    # 1) What users like
    if "what do users like" in lower or "what users like" in lower or "what people like" in lower:
        positive = [r for r in all_reviews if "positive" in r["sentiment_label"]]
        sample = positive[:100]
        sample_texts = [r["full_text"] for r in sample]

        prompt = (
            "You are Ana, a T-Mobile Data Analyst.\n"
            "Based ONLY on the following positive/slightly positive reviews, "
            "summarize the top themes of what users like in 5 bullet points.\n\n"
            + "\n\n".join(sample_texts)
        )
        try:
            resp = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
            )
            reply = getattr(resp, "text", None) or \
                "Users appreciate network quality, staff interactions, and available promotions."
            return jsonify({"response": reply}), 200
        except Exception as e:
            logger.error(f"‚ùå /chat like-summary error: {e}")
            return jsonify({"response": "Error summarizing liked aspects."}), 500

    # 2) Complaints / issues
    if any(w in lower for w in ["complaint", "complaints", "issues", "problems"]):
        negative = [r for r in all_reviews if "negative" in r["sentiment_label"]]
        sample = negative[:100]
        sample_texts = [r["full_text"] for r in sample]

        prompt = (
            "You are Ana, a T-Mobile Data Analyst.\n"
            "Based ONLY on the following negative/slightly negative reviews, "
            "summarize the main complaints in 5 bullet points.\n\n"
            + "\n\n".join(sample_texts)
        )
        try:
            resp = gemini_client.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
            )
            reply = getattr(resp, "text", None) or \
                "Customers mention service reliability, store experience, and billing clarity."
            return jsonify({"response": reply}), 200
        except Exception as e:
            logger.error(f"‚ùå /chat complaints-summary error: {e}")
            return jsonify({"response": "Error summarizing complaints."}), 500

    # 3) Top / best locations
        # ---- Shortcut 3: "Top locations / places / stores" ----
    if (
        ("top" in lower and any(w in lower for w in ["location", "locations", "place", "places", "store", "stores"]))
        or ("best locations" in lower)
        or ("top 5 places" in lower)
        or ("top 5 stores" in lower)
    ):
        # Build location stat
        
        location_map = {}
        for r in all_reviews:
            city = r.get("city")
            state = r.get("state")
            if not city:
                continue
            key = f"{city}, {state}" if state else city
            if key not in location_map:
                location_map[key] = {"pos": 0, "neg": 0, "total": 0}
            location_map[key]["total"] += 1
            lbl = r["sentiment_label"]
            if "positive" in lbl:
                location_map[key]["pos"] += 1
            if "negative" in lbl:
                location_map[key]["neg"] += 1

        ranked = sorted(
            location_map.items(),
            key=lambda kv: ((kv[1]["pos"] / kv[1]["total"]) if kv[1]["total"] else 0, kv[1]["pos"]),
            reverse=True,
        )
        top_n = ranked[:5]

        if not top_n:
            return jsonify({"response": "I couldn't compute top locations from the data."}), 200

        lines = []
        for i, (loc, stats) in enumerate(top_n, start=1):
            pct = (stats["pos"] / stats["total"]) * 100 if stats["total"] else 0
            lines.append(
                f"{i}. {loc} ‚Äì {stats['pos']} positive out of {stats['total']} reviews ({pct:.0f}% positive)"
            )

        reply = "Here are the top performing locations based on sentiment:\n" + "\n".join(lines)
        return jsonify({"response": reply}), 200

    # 4) Random city
    if "random city" in lower:
        if city_names:
            city = random.choice(city_names)
            return jsonify({"response": f"Here's a random city from the dataset: {city}"}), 200
        return jsonify({"response": "I don't have any city names available in the data."}), 200

    # 5) Specific city (e.g., Dallas)
    for city in city_names:
        if city and city.lower() in lower:
            city_reviews = [r for r in all_reviews if str(r.get("city", "")).lower() == city.lower()]
            if city_reviews:
                pos = sum(1 for r in city_reviews if "positive" in r["sentiment_label"])
                neg = sum(1 for r in city_reviews if "negative" in r["sentiment_label"])
                neu = len(city_reviews) - pos - neg
                reply = (
                    f"In {city}, based on {len(city_reviews)} reviews: "
                    f"{pos} positive, {neg} negative, {neu} neutral/slightly mixed."
                )
                return jsonify({"response": reply}), 200

    # 6) Fallback: grounded Gemini over summary + sample
    if not use_latest:
        system_instruction = (
            "You are Ana, a helpful T-Mobile assistant. Respond briefly and professionally."
        )
        try:
            session = gemini_client.chats.create(
                model="gemini-2.5-flash",
                config={"system_instruction": system_instruction},
            )
            resp = session.send_message(message)
            reply = getattr(resp, "text", None) or "I couldn't generate a response."
            return jsonify({"response": reply}), 200
        except Exception as e:
            logger.error(f"‚ùå /chat ungrounded error: {e}")
            return jsonify({"response": f"Server error: {e}"}), 500

    sample = all_reviews[:80]
    context = (
        "You are Ana, a T-Mobile Data Analyst Bot. "
        "Use ONLY the JSON data below to answer questions. "
        "If something is not supported by these data, say you cannot find it.\n\n"
        "DATA SUMMARY:\n"
        f"{json.dumps(summary, indent=2, ensure_ascii=False)}\n\n"
        "SAMPLE ENTRIES:\n"
        f"{json.dumps(sample, indent=2, ensure_ascii=False)}"
    )

    try:
        session = gemini_client.chats.create(
            model="gemini-2.5-flash",
            config={"system_instruction": context},
        )
        resp = session.send_message(message)
        reply = getattr(resp, "text", None) or "I couldn't generate a response."
        return jsonify({"response": reply}), 200
    except Exception as e:
        logger.error(f"‚ùå /chat fallback error: {e}")
        return jsonify({"response": f"Server error: {e}"}), 500

# ==========================
# üöÄ Run
# ==========================

if __name__ == "__main__":
    logger.info("üöÄ Project Ana server starting...")
    logger.info(f"Using .env from: {env_path}")
    app.run(host="0.0.0.0", port=5000, debug=True)
